---
title: "Introdución a la Regresión Lineal"
author: "Santiago Pérez Moncada"
date: "24/7/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Seguramente, en algún momento de nuestra vida ya sea hojeando un libro de matemáticas, curioseando artículos científicos...haz visto una línea recta o algún tipo de curva en un gráfico que se ajusta a las observaciónes representadas por medio de puntos en el plano.

En general, la situación es la siguiente:Supongamos que tenemos una serie de puntos en el plano cartesiano $\mathbb{R}^2$ de la forma.

$$(x_{1},y_{1}),...,(x_{n},y_{n})$$

que representan las observaciones de dos variable numéricas. Digamos que $x$ es la edad e $y$ el peso de $n$ estudiantes.

**Nuestro objetivo:** Describir la relación entre la **variable independiente**, $x$, y la **variable dependiente**, $y$, apartir de estas observaciones.

Para ello, lo que haremos será buscar una función $y=f(x)$ cuya gráfica se aproxime lo máximo posible a nuestro pares ordenados $(x_{i},y_{i})\;\;\; i=1,...,n$

Esta función nos dará un modelo matemático de cómo se comportan estas observaciones, lo cual nos permitirá entender mejor los mecanismos que relacionan las variables estudiadad o incluso, nos dara la oportunidad de hacer predicciones sobre futuras observaciones.


El modelo lineal es el modelo mas simple que existe. Consiste en estudiar si los puntos $(x_{i},y_{i})\;\;\; i=1,...,n$ satisfacen una relación de la forma.

$$y = ax + b \hspace{1em} a,b \in \mathbb{R}$$

En este caso, se busca la recta $y=ax+b$ que mejor se aproxime a los puntos dados imponiendo que la suma de los cuadrados de las diferencias entre sus valore $y_{i}$ y sus aproximaciones $\tilde{y_{i}} = ax_{i} + b$ sea mínima. Es decir que

$$\sum_{i=1}^n(y_{i}-\tilde{y_{i}})^2$$
El objetivo de este tema no es más que enseñarnos hacer uso de R para obtener esa recta de regresión.

Veremos tambien cómo se puede evaluar numéricamente si esta recta se ajusta bien a las observaciones dadas.

Para ello, introduciremos algunas funciones de R y haremos uso de trasformaciones logarítmicas para tratar casos en los que los puntos dados se aproximen mejor mediante una función exponencial o potencial.

# Planteamiento del problema

Como ya hemos dicho, el objetivo de este tema es estudiar si existe relación lineal entre las variables dependiente e independiente.

Por lo general, cuando tenemos una serie de observaciones emparejadas, $(x_{i},y_{i})\;\;\; i=1,...,n$, la forma natural de almacenarlas en R es mediante una tabla de datos. Y la que más conocemos es un data frame.

Como recordaras de temas anteriores, la ventaja de trabajar con este tipo de organización de datos es que luego se pueden hacer muchas cosas.

**Usaremos los datos de body fat**

```{r}
body = read.table("../data/bodyfat.txt", header = TRUE)
head(body,3)
```

**Más concretamente, trabajaremos con las variables `fat` y `weight`**

```{r}
body2 = body[,c(2,4)]
names(body2) = c("Grasa","Peso")
str(body2)
head(body2)
```

Al analizar datos, siempre es recomendable empezar con una representación gráfica que nos permita hacernos a la idea de lo que tenemos.

Esto se consigue haciendo uso de la función `plot`, que ya hemos estudiado a detalle en lecciones anteriores. No obstante, para lo que necesitamos en este tema nos conformaremos con un gráfico básico de estos puntos que nos muestre su distribución.

```{r}
plot(body2)
```

Para calcular la **recta de regresión** con `R` de la familia de puntos $(x_{i},y_{i})\;\;\; i = 1,...,n$ si $\vec{x}$ es el vector de datos $\vec{x}=(x_{1},...,x_{n})$ e $\vec{y}$ es el vector de datos $\vec{y} = (y_{1},...,y_{n})$, entonces su recta de regresión en `R` se calcula mediante la instrucción `lm(y~x)`

**Cuidado con la sintaxis:** primero va el vector de las variables dependientes y, segudamente después de una tilde ~, va el vector de las variables independientes.

Esto se debe a que `R` toma el signidicado de la tilde como "en función de" . Es decir, la interpretación de `lm(y~x)` en `R` es la recta de regresión de $y$ en función de $x$.

Si los vectores $\vec{x}$ e $\vec{y}$ son, en este orden, la primera y la segunda columna de un data frame de dos variables, entonces es suficiente aplicar la función `lm` al data frame.

En general, si $\vec{x}$ e $\vec{y}$ son dos variables de un data frames, para calcular la recta de regresión de $\vec{y}$ en función de $\vec{x}$ podemos usar la instrucción. `lm(y~x, data=DataFrame)`.


```{r}
lm(body2$Peso~body2$Grasa) #Opcion 1
lm(Peso~Grasa, data = body2)#Opcion 2
```

Como podemos observar, las dos formas de llamar a la función `lm` dan exactamente los mismo.

El resultado obtenido en ambos caso significa que la recta de regresión para nuestro datos es.

$$\tilde{y} = 2.151x + 137.738$$

donde $y$ es el peso y $x$ es la grasa. Ahora podemos superponer esta recta a nuestro gráfico anterior haciendo uso de la función `abline()`.

```{r}
plot(body2)
abline(lm(Peso~Grasa, data = body2), col = "red")
```

Hay que tener en cuenta que el análisis llevado a cabo hasta el momento de los pares $(x_{i},y_{i})\;\;\; i = 1,...,n$ ha sido pueramente descriptivo.

Es decir, hemos mostrado que los datos son consistente con una función lineal, pero no hemos mostrado que la variable dependiente sea función aproximadamente lineal de la variable independiente.Esto último necesitaría una demostración matemática, o bien un argumento biológico, pero no basta con una simple comprobación numérica.

**Eso si, podemos utilizar lo hecho hasta ahora para predecir valores** $\tilde{y_{i}}$ en función de los valores $x_{i}$ resolviendo una simple ecuación lineal.

# Coeficiente de Determinación

El coeficiente de determinación, $R^2$, nos es útil para evaluar numéricamente si la relación lineal obtenida es significativa o no.

No explicaremos de momento como se define.Eso lo dejamos para curiosidad del usuario.Por el momento es suficiente, con saber que este coeficiente se encuentra en el intervalo $[0,1]$. Si $R^2 > 0.9$ , consideramos que el ajuste es bueno. De lo contrario, no.

La funcion `summary` aplicada a `lm` nos muestra los contenidos de este objeto. Entre  ellos encontraremos `Multiple R-squared`, que no es ni más ni menos que el coeficiente de determinación $R^2$.

Para facilitarnos las cosas y ahorrarnos información que, de momento no nos resulta de interés, podemos aplicar `summary(lm(...))$r.squared`.

```{r}
summary(lm(Peso~Grasa, data=body2))
```

Si hacemos la siguiente instrucción vamos al elemento que nos interesa.

```{r}
summary(lm(Peso~Grasa, data=body2))$r.squared
```

**En este caso hemos obtenido un coeficiente de determinación de 0.3751 cosa que confirma que la recta de regresión no aproxima nada bien nuestros datos.**


# Tranformaciones Logarítmicas

No siempre encontraremos dependencias lineales. Aveces nos encontraremos otro tipo de dependencias, como por ejemplo potencias o exponenciales.

Estas se pueden transformar a lineales mediante un **cambio de escala**.

Por lo general, es habitual encontrarnos gráficos con sus ejes en una **escala lineal**. Es decir, las marcas de los ejes estan igualmente espaciadas.

A veces, es conveniente dibujar alguno de los ejes en **escala logarítmica**, de modo que la misma distancia entre las marcas significa el mismo cociente entre sus valores.En otras palabrasm un eje en escala logaritmica representa el **logaritmo** de sus valores en escala lineal.

Diremos que un gráfico está en **escala semilogaritmica** cuando su eje de abcisas está en escala lineal y el de las ordenadas, en escala logaritmica.

Diremos que un gráfico está en **escala doble logarítmica** cuando ambos ejes están es escala logarítmica.

## Interpretación Gráfica

Si al representar unos puntos $(x_{i},y_{i})\;\;\;i=1,...,n$ en escala semilogarítmica observamos que siguen aproximadamente una recta, esto querrá decir que los valores $\log(y)$ siguen una ley aproximadamente lineal en los valores $x$ y por lo tanto $y$ sigue una **ley aproximadamente exponencial** en $x$.

En efecto si $\log(y) = ax+b$, entonces,

$$y = 10^{\log(y)} = 10^{ax+b}=10^{ax}\cdot10^b = \alpha^x \beta$$

con $\alpha = 10^a$ y $\beta = 10^b$

SI al representar unos puntos $(x_{i},y_{i})\;\;\;i=1,...,n$ en escala doble logaritmica observamos que siguen aproximadamente una recta, esto querra decir que los valores $\log(y)$ siguen una ley aproximadamente lineal en los valores $\log(x)$, y por lo tanto que $y$ sigue una **ley aproximadamente potencial** en $x$.

En efecto, si $\log(y) = a\log(x)+b$, entonces, por propiedades de logaritmos.

$$y = 10^{\log(y)} = 10^{a\log(x)+b} = (10^{\log(x)})^a \cdot 10^b = x^a\beta$$

con $\beta = 10^b$

## Ejemplo

En este caso trabajaremos no con un data frame, si no directamente con los dos siguientes vectores.

```{r}
dep = c(1.2,3.6,12,36)
ind = c(20,35,61,82)
plot(ind,dep, main = "Escala lineal")
plot(ind,dep, log = "y", main = "Escala semi-logarítmica")

lm(log10(dep)~ind)
summary(lm(log10(dep)~ind))$r.squared
```

Podemos afirmar que en efecto el modelo lineal que ajusta el logaritmo en base 10 de la variable dependiente en función de la independiente es en efecto una recta.

Lo que acabamos de obtener es que

$$\log(dep) = 0.023\cdot ind - 0.33$$
es una buena aproximación  de nuestro datos.
Con lo cual obtenemos el siguiente modelo exponencial.

$$dep = 10^{0.023\cdot ind} \cdot 10^{-0.33} = 1.054^{ind} \cdot 0.468$$

```{r}
plot(ind,dep, main = "Curva de regresión")
curve(1.054^x * 0.468, add = TRUE, col="purple")
```

## Ejemplo

```{r}
tiempo = 1:10
gramos = c(0.097,0.709,2.698,6.928,15.242,29.944,52.902,83.903,120.612,161.711)
d.f = data.frame(tiempo,gramos)

plot(d.f, main = "escala lineal")
plot(d.f, log="y", main = "escala semi-log")
plot(d.f,log = "xy", main = "escala doble-log")

lm(log10(gramos)~log(tiempo), data = d.f)
summary(lm(log10(gramos)~log(tiempo), data = d.f))$r.squared
```

Lo que acabamos de obtener es que

$$\log(gramos) = 3.298\cdot\log(tiempo) - 1.093$$

es una buena aproximación que sigue un **modelo potencial.**
Con lo cual

$$gramos = 10^{3.298\cdot\log(tiempo)}\cdot10^{-1.093} = tiempo^{3.298}\cdot 0.081$$

```{r}
plot(d.f, main = "Curva de regresión")
curve(x^(3.298)*0.081, add = TRUE, col="purple")
```

